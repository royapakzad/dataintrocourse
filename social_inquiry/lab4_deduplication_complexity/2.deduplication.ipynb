{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deduplication part 1\n",
    "\n",
    "One important, and potentially expensive (in terms of efficiency and energy consumption), task when handling data is *data deduplication*.  The goal of this task is to make sure that each item in your data set (e.g., each person) only appears once.  We'll explore 3 different ways this can be done in this lab (in this part and in part 3), and determine which one is most efficient.\n",
    "\n",
    "First, let's examine some new data!  The data we'll be working with comes from a ProPublica story about a risk assessment tool called COMPAS.  In order to understand the data, start by reading the article.  The data can be found in this directory at \"compas-scores.csv\".\n",
    "\n",
    "** 1) Read the ProPublica article <https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing> and the document describing how they did their analysis <https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2) What does each row in the dataset represent?  Would it make sense for there to be duplicate items? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3) Get the data using the function we used in past labs. **\n",
    "\n",
    "You'll want to use this function instead of using a csv reader so that when you need to go over the data multiple times (below) you don't have to keep reopening the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a filename and returns your data.\n",
    "For example, with a file that looks like this:\n",
    "header1, header2, header3\n",
    "1,2,3\n",
    "4,5,6\n",
    "\n",
    "You could get the first row, second header item like this:\n",
    "dict = get_data(\"temp.csv\")\n",
    "print dict[1][\"header2\"]\n",
    "\n",
    "For the interested, the returned data is a list of dictionaries.  We'll see this more in future weeks.\n",
    "\"\"\"\n",
    "def get_data(filename):\n",
    "    filepointer = open(filename, \"r\")\n",
    "    data = []\n",
    "    \n",
    "    # get_header, inline instead of calling the function above so that the file continues reading\n",
    "    # from the line right after the header in the for loop below.\n",
    "    line = filepointer.readline()\n",
    "    header = line.strip().split(\",\")\n",
    "\n",
    "    for line in filepointer:\n",
    "        fields = line.strip().split(\",\")\n",
    "\n",
    "        # Unfortunately, split will split at some commas that we don't mean to split on (e.g., if they've\n",
    "        # been written into addresses) so we check below to make sure we have the expected number of fields\n",
    "        # and throw out any other data.  We shouldn't really be throwing out data, we should be fixing the\n",
    "        # actual problem, but for the purposes of this lab, this will do.\n",
    "        if (len(fields) == len(header)):\n",
    "            row = {}\n",
    "            for fieldNumber in range(len(fields)):\n",
    "                row[header[fieldNumber]] = fields[fieldNumber]\n",
    "            data.append(row)\n",
    "            \n",
    "    filepointer.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine if two rows contain the same item, we need to develop a function that checks the information that *should* be unique to each row and compares it to determine equality.  We need to decide what information this should be carefully, or deduplication will fail.  For example, the compas-scores.csv file has an \"id\" column, which seems at first glance like it should be unique in the file (and it is), but these were determined for the purposes of this file and so duplicate entries will have different ids.  We need to instead choose fields (potentially multiple fields) that are unique per person and check equality based on those.\n",
    "\n",
    "** 4) Write a function that takes a row and returns a string version of the row containing only the fields that should be used to check equality. **\n",
    "\n",
    "** 5) Write a function takes two rows as input and returns True if these rows are the same and False otherwise. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to find the number of duplicates in a data set is to compare each item to each other item in the data set, counting duplicates as you go.  Make sure *not* to count the comparison of an item to itself as a duplicate.\n",
    "\n",
    "** 6) Create a function that uses this \"all pairs\" method of deduplication to return the number of duplicate items in a given data set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the number of duplicates is to create a key for each item in the data set and insert these items into a dictionary, with the count of the number of times this item is seen as the value.  The choice of key will determine whether two items are considered to be duplicates, so choose it carefully.  Note that this function and the previous duplicate counting function should return the same number of duplicates.\n",
    "\n",
    "** 7) Create a function that uses this dictionary-based method of deduplication to return the number of duplicate items in a given data set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 8) Using the module you developed from the previous part of the lab, determine: **\n",
    "\n",
    "a) How many seconds does each duplication counting method take when run on the ProPublica data set?\n",
    "    \n",
    "b) How many items (e.g., cups of coffee) could be created by this amount of energy for each duplication counting method?\n",
    "    \n",
    "c) Create two graphs using regplot showing the number of rows deduplicated on the x-axis and the energy consumption in number of items on the y-axis for each of these methods.  Note that this means you'll need to create multiple versions of the dataset with increasing numbers of rows to be checked.  (For *Extra Credit* graph these on the same plot using lmplot or another seaborn method.)\n",
    "    \n",
    "d) Explain which method is more efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
